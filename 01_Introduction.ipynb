{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The basics\n",
    "\n",
    "##### Keras and Tensorflow\n",
    "Deep Learning is based on lots of ugly math: while Tensorflow is a library that allows you to efficiently handle such math, Tensorflow code can still be really obscure to read. Keras provides a nice, readable interface to build Neural Networks, where the user can focus on Network design and let Keras worry about the math and data flow behind the scenes. (Note: Tensorflow 2 is actually fully embracing the idea of using Keras as a frontend)\n",
    "\n",
    "##### Data preparation: numpy arrays shape\n",
    "When doing deep learning, 90% of the errors are about shape mismatch. So, before we even start, it is worth to have a look at what data format will Keras expect from us, when used in combination with Tesorflow (some things are different with others backends). We will feed numpy arrays to our networks, and the following table will be a useful reference for the shape of our data:\n",
    "\n",
    "| Shape         | Batch size    | data format      |\n",
    "| :---:         | :---:         | :---:            |\n",
    "| (N, m)        | N             | m-dim vectors    |\n",
    "| (N, m, p)     | N             | m x p matrices   |\n",
    "| (None, m, p)  | unknown       | m x p matrices   |\n",
    "| (N, m, p, 3)  | N             | m x p RGB images |\n",
    "\n",
    "\n",
    "##### The basic DL unit: a single neuron\n",
    "A neural network is a network of neurons, where each neuron performs the following operation:\n",
    "\n",
    "` y = Neuron(x) = Ax + b`\n",
    "\n",
    "In the simplest case, if the input x is a m-dim vectors, A will be a matrix 1xm (**weights**), b a scalar (**bias**), and the neuron will output a scalar value. *Training* such neuron would be the equivalent of performing a linear fit.\n",
    "\n",
    "Let's write this in Keras!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write python code below\n",
    "\n",
    "# stuff we will need\n",
    "\n",
    "\n",
    "# build a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(2,10) # refer to table above for shape\n",
    "print(model.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, b = model.get_weights()\n",
    "print(np.dot(x,A) + b) # note xA because of shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let's make a Neural Network\n",
    "If we put together more neurons we get a Network: let's build a new Sequential model with a few layers.\n",
    "We want something that looks like this:\n",
    "![nn](nn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write python code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how quickly we wrote a fit model with 70 fit parameters! The only thing left is to actually start fitting data..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
